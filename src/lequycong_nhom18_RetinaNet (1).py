# -*- coding: utf-8 -*-
"""Báº£n_sao_cá»§a_VinChest_Xray.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/193mAJOxCdA-X8UU-Fqaii5tyykpnBw7j
"""

from IPython.display import Image
from google.colab import files

!pip install -q kaggle

files.upload() # Upload kaggle.json file that was downloaded

!mkdir '/root/.kaggle'
!cp kaggle.json '/root/.kaggle'
!chmod 600 /root/.kaggle/kaggle.json # Sets the appropriate permissions for the kaggle.json file using the !chmod shell command

!kaggle datasets list # Displays the list of available datasets on Kaggle to check if everything is functioning properly

!kaggle datasets download awsaf49/vinbigdata-512-image-dataset

!unzip -q vinbigdata-512-image-dataset.zip

# !cp /content/vinbigdata-512-image-dataset.zip /content/drive/MyDrive/

!pip install bbox_visualizer

import numpy as np
import pandas as pd
import os
from sklearn.model_selection import train_test_split
from sklearn import model_selection

import cv2
import matplotlib.pyplot as plt
import seaborn as sns
import bbox_visualizer as bbv

# # import pydicom
# from pydicom.pixel_data_handlers.util import apply_voi_lut
from glob import glob
# from skimage import exposure

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torch.utils.data.sampler import SequentialSampler
import torchvision
from torchvision.models.detection import retinanet_resnet50_fpn
from torchvision.models.detection.retinanet import RetinaNet

import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2

import warnings

warnings.filterwarnings('ignore')

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

BASE_DIR = "/content/vinbigdata"
df = pd.read_csv(os.path.join(BASE_DIR, "train.csv"))
train =df.copy()
df.head()

plt.figure(figsize=(8, 8))

sns.countplot(train.class_name)
plt.xticks(fontsize=14,rotation=90)
plt.yticks(fontsize=14)
plt.title("Distribution of labels in Annotation dataframe", fontsize=16);

train = train[train.class_name!='No finding'].reset_index(drop=True)

plt.figure(figsize=(8, 8))

sns.countplot(train.class_name)
plt.xticks(fontsize=14,rotation=90)
plt.yticks(fontsize=14)
plt.title("Distribution of labels in Annotation dataframe", fontsize=16);

# áº¢nh background (No finding)
bg_ids = df[df['class_name'] == 'No finding']['image_id'].unique()
obj_ids = df[df['class_name'] != 'No finding']['image_id'].unique()
print("Sá»‘ áº£nh cÃ³ object:", len(obj_ids))
print("Sá»‘ áº£nh background:", len(bg_ids))

CLASS_NAMES = [
  'Aortic enlargement',
  'Atelectasis',
  'Calcification',
  'Cardiomegaly',
  'Consolidation',
  'ILD',
  'Infiltration',
  'Lung Opacity',
  'Nodule/Mass',
  'Other lesion',
  'Pleural effusion',
  'Pleural thickening',
  'Pneumothorax',
  'Pulmonary fibrosis'
]

"""Normalize theo ImageNet"""

import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2

IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD  = (0.229, 0.224, 0.225)

def get_train_transform():
    return A.Compose([
        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=1.0),
        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD, max_pixel_value=255.0),
        ToTensorV2(),
    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})

def get_valid_transform():
    return A.Compose([
        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD, max_pixel_value=255.0),
        ToTensorV2(),
    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})


class ChestAnnotationDataset(Dataset):
    def __init__(self, dataframe, image_dir, transforms=None):
        super().__init__()
        self.image_ids = dataframe['image_id'].unique()
        self.df = dataframe
        self.image_dir = image_dir
        self.transforms = transforms

    def __getitem__(self, index: int):
        image_id = self.image_ids[index]
        records = self.df[self.df['image_id'] == image_id]

        # ---- READ PNG ----
        image_path = f"{self.image_dir}/{image_id}.png"
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

        if image is None:
            raise ValueError(f"Cannot read image: {image_path}")

        # convert 1 channel -> 3 channels
        image = np.stack([image, image, image], axis=0)  # shape (3, H, W)

        image = image.astype("float32")

        # convert to (H, W, 3) for Albumentations
        image = image.transpose(1, 2, 0)

        # ---- BOXES & TARGETS ----
        original_W = records.width.values[0]
        original_H = records.height.values[0]
        current_H, current_W, _ = image.shape

        scale_x, scale_y = 1.0, 1.0
        if original_W > 0 and original_H > 0:
            scale_x = current_W / original_W
            scale_y = current_H / original_H

        scaled_x_min = records['x_min'].values * scale_x
        scaled_y_min = records['y_min'].values * scale_y
        scaled_x_max = records['x_max'].values * scale_x
        scaled_y_max = records['y_max'].values * scale_y

        # Clip coordinates to image boundaries
        scaled_x_min = np.clip(scaled_x_min, 0, current_W)
        scaled_y_min = np.clip(scaled_y_min, 0, current_H)
        scaled_x_max = np.clip(scaled_x_max, 0, current_W)
        scaled_y_max = np.clip(scaled_y_max, 0, current_H)

        scaled_boxes = np.stack([scaled_x_min, scaled_y_min, scaled_x_max, scaled_y_max], axis=1)

        # Filter out invalid boxes (width or height <= 0)
        valid_boxes_mask = (scaled_boxes[:, 2] > scaled_boxes[:, 0]) & \
                           (scaled_boxes[:, 3] > scaled_boxes[:, 1])

        if len(valid_boxes_mask) == 0 or not np.any(valid_boxes_mask):
            # No valid boxes, create empty tensors
            boxes_tensor = torch.zeros((0, 4), dtype=torch.float32)
            labels_tensor = torch.zeros((0,), dtype=torch.int64)
            area_tensor = torch.zeros((0,), dtype=torch.float32)
            iscrowd_tensor = torch.zeros((0,), dtype=torch.int64)
        else:
            valid_boxes = scaled_boxes[valid_boxes_mask]
            valid_labels = records.class_id.values[valid_boxes_mask] + 1
            valid_iscrowd = torch.zeros((len(valid_boxes)), dtype=torch.int64)

            boxes_tensor = torch.tensor(valid_boxes, dtype=torch.float32)
            labels_tensor = torch.tensor(valid_labels, dtype=torch.int64)
            area_tensor = (boxes_tensor[:, 3] - boxes_tensor[:, 1]) * \
                          (boxes_tensor[:, 2] - boxes_tensor[:, 0])
            iscrowd_tensor = valid_iscrowd

        target = {
            "boxes": boxes_tensor,
            "labels": labels_tensor,
            "area": area_tensor,
            "iscrowd": iscrowd_tensor
        }

        # ---- TRANSFORMS ----
        if self.transforms:
            sample = {
                "image": image,
                "bboxes": target["boxes"].numpy(),
                "labels": target["labels"].numpy()
            }
            sample = self.transforms(**sample)
            image = sample["image"]
            target["boxes"] = torch.tensor(sample["bboxes"], dtype=torch.float32)
            target["labels"] = torch.tensor(sample["labels"], dtype=torch.int64)

        return image, target

    def __len__(self):
        return len(self.image_ids)

def collate_fn(batch):
    return tuple(zip(*batch))

all_images = np.concatenate([obj_ids, bg_ids])
labels = np.concatenate([
    np.ones(len(obj_ids)),  # object = 1
    np.zeros(len(bg_ids))   # background = 0
])

train_imgs, valid_imgs = train_test_split(
    all_images,
    test_size=0.2,
    shuffle=True,
    stratify=labels,
    random_state=42
)

train_df = df[df["image_id"].isin(train_imgs)]
valid_df = df[df["image_id"].isin(valid_imgs)]

DIR_TRAIN = os.path.join(BASE_DIR, "train")
train_dataset = ChestAnnotationDataset(train_df, DIR_TRAIN, get_train_transform())
valid_dataset = ChestAnnotationDataset(valid_df, DIR_TRAIN, get_valid_transform())


train_loader = DataLoader(
    train_dataset,
    batch_size=16,
    shuffle=True,
    num_workers=4,
    collate_fn=collate_fn
)

valid_loader = DataLoader(
    valid_dataset,
    batch_size=16,
    shuffle=False,
    num_workers=4,
    collate_fn=collate_fn
)

images, targets = next(iter(train_loader))
images = list(image.to(device) for image in images)

targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

print("Successfully loaded a batch of images and targets.")

def visualize_plot(idx, images, targets):
    class_types = {
        0: 'Aortic enlargement',
        1: 'Atelectasis',
        2: 'Calcification',
        3: 'Cardiomegaly',
        4: 'Consolidation',
        5: 'ILD',
        6: 'Infiltration',
        7: 'Lung Opacity',
        8: 'Nodule/Mass',
        9: 'Other lesion',
        10: 'Pleural effusion',
        11: 'Pleural thickening',
        12: 'Pneumothorax',
        13: 'Pulmonary fibrosis'
    }

    # Boxes & labels
    boxes = targets[idx]['boxes'].cpu().numpy().astype(np.int32)
    labels = (targets[idx]['labels'] - 1).cpu().numpy()

    # ---- DENORMALIZE ----
    img = images[idx].cpu().permute(1, 2, 0).numpy()  # (HWC)
    img = img * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN)
    img = (img * 255).clip(0, 255).astype(np.uint8)

    img_draw = img.copy()

    # ---- DRAW BOXES ----
    for box, label in zip(boxes, labels):
        class_name = class_types[int(label)]

        # draw bbox
        cv2.rectangle(
            img_draw,
            (box[0], box[1]),
            (box[2], box[3]),
            (255, 0, 0),
            3
        )

        # draw label
        bbv.add_label(
            img_draw,
            class_name,
            box,
            draw_bg=True,
            text_bg_color=(255, 0, 0),
            text_color=(0, 0, 0),
        )

    # ---- PLOT ----
    plt.figure(figsize=(12, 12))
    plt.imshow(img_draw)
    plt.axis("off")
    plt.show()

visualize_plot(5,images,targets)

from torchvision.models.detection import retinanet_resnet50_fpn_v2
from torchvision.models.detection.retinanet import RetinaNetClassificationHead

def get_retinanet(num_classes):
    model = retinanet_resnet50_fpn_v2(weights="DEFAULT")

    # Extract number of anchors (A)
    anchor_generator = model.anchor_generator
    num_anchors = anchor_generator.num_anchors_per_location()[0]

    # Replace classification head
    in_channels = model.backbone.out_channels
    model.head.classification_head = RetinaNetClassificationHead(
        in_channels=in_channels,
        num_anchors=num_anchors,
        num_classes=num_classes
    )

    # Freeze backbone (ResNet + FPN)
    for p in model.backbone.parameters():
        p.requires_grad = False

    print("Backbone frozen")
    return model

from tqdm import tqdm

def train_one_epoch(model, optimizer, dataloader, device):
    model.train()
    epoch_loss = 0
    epoch_cls = 0
    epoch_reg = 0

    pbar = tqdm(dataloader, total=len(dataloader))

    for images, targets in pbar:
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        # Forward
        loss_dict = model(images, targets)
        loss_cls = loss_dict["classification"].item()
        loss_reg = loss_dict["bbox_regression"].item()
        loss = sum(loss_dict.values())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Accumulate
        epoch_loss += loss.item()
        epoch_cls += loss_cls
        epoch_reg += loss_reg

        # Update tqdm
        pbar.set_description(
            f"[Train] Total: {loss.item():.4f} | Cls: {loss_cls:.4f} | Reg: {loss_reg:.4f}"
        )

    n = len(dataloader)
    return {
        "total": epoch_loss / n,
        "cls": epoch_cls / n,
        "reg": epoch_reg / n,
    }

!pip install torchmetrics

from torchmetrics.detection.mean_ap import MeanAveragePrecision

def evaluate_map(model, dataloader, device):
    model.eval()
    metric = MeanAveragePrecision(iou_type="bbox").to(device)

    with torch.no_grad():
        for images, targets in dataloader:
            images = [img.to(device) for img in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            outputs = model(images)  # inference mode

            # Convert output format â†’ torchmetrics format
            preds = []
            for o in outputs:
                preds.append({
                    "boxes": o["boxes"],
                    "scores": o["scores"],
                    "labels": o["labels"],
                })

            gt = []
            for t in targets:
                gt.append({
                    "boxes": t["boxes"],
                    "labels": t["labels"]
                })

            metric.update(preds, gt)

    result = metric.compute()
    return {
        "mAP50": result["map_50"].item(),
        "mAP50_95": result["map"].item()
    }

def val_one_epoch(model, dataloader, device):
    model.train()
    epoch_loss = 0
    epoch_cls = 0
    epoch_reg = 0

    with torch.no_grad():
        pbar = tqdm(dataloader, total=len(dataloader))

        for images, targets in pbar:
            images = [img.to(device) for img in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            loss_dict = model(images, targets)
            loss_cls = loss_dict["classification"].item()
            loss_reg = loss_dict["bbox_regression"].item()
            loss = sum(loss_dict.values())

            epoch_loss += loss.item()
            epoch_cls += loss_cls
            epoch_reg += loss_reg

            pbar.set_description(
                f"[VAL] Total: {loss.item():.4f} | Cls: {loss_cls:.4f} | Reg: {loss_reg:.4f}"
            )

    n = len(dataloader)
    return {
        "total": epoch_loss / n,
        "cls": epoch_cls / n,
        "reg": epoch_reg / n,
    }

import os
import torch

def save_checkpoint(model, optimizer, epoch, history, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)

    ckpt = {
        "epoch": epoch,
        "model_state": model.state_dict(),
        "optimizer_state": optimizer.state_dict(),
        "history": history
    }
    torch.save(ckpt, path)
    print(f"Checkpoint saved to {path}")

def load_checkpoint(model, optimizer, path, device):
    ckpt = torch.load(path, map_location=device)

    model.load_state_dict(ckpt["model_state"])
    optimizer.load_state_dict(ckpt["optimizer_state"])

    print(f"Loaded checkpoint from {path}, epoch = {ckpt['epoch']}")
    return ckpt["epoch"], ckpt["history"]

def freeze_backbone(model):
    for param in model.backbone.parameters():
        param.requires_grad = False
    print("!!! Backbone frozen.")


def unfreeze_backbone_partial(model, layers=("layer3", "layer4")):
    # Unfreeze cuá»‘i cá»§a ResNet
    for name, param in model.backbone.body.named_parameters():
        if any(layer in name for layer in layers):
            param.requires_grad = True
    print("!!! Unfroze layers:", layers)

def build_optimizer(model, lr=1e-4):
    params = [p for p in model.parameters() if p.requires_grad]
    return torch.optim.AdamW(params, lr=lr)

history = {
        "train_total": [], "train_cls": [], "train_reg": [],
        "val_total": [], "val_cls": [], "val_reg": [],
        "mAP50": [], "mAP50_95": []
}

def train_model(model, train_loader, val_loader, num_epochs,
                freeze_epochs=5, exp_dir="exp"):

    os.makedirs(exp_dir, exist_ok=True)

    # ---- Stage 1: Freeze backbone ----
    freeze_backbone(model)
    optimizer = build_optimizer(model, lr=1e-4)

    best_map50 = 0.0
    model.to(device)

    for epoch in range(num_epochs):
        print(f"\n===== Epoch {epoch+1}/{num_epochs} =====")

        # ---- Stage 2: Unfreeze partial backbone ----
        if epoch == freeze_epochs:
            unfreeze_backbone_partial(model, layers=("layer3", "layer4"))
            optimizer = build_optimizer(model, lr=5e-5)
            print("âš¡ Optimizer rebuilt for fine-tuning.")

        # ---- TRAIN ----
        train_stats = train_one_epoch(model, optimizer, train_loader, device)

        history["train_total"].append(train_stats["total"])
        history["train_cls"].append(train_stats["cls"])
        history["train_reg"].append(train_stats["reg"])

        # ---- VALIDATION LOSS ----
        val_stats = val_one_epoch(model, val_loader, device)

        history["val_total"].append(val_stats["total"])
        history["val_cls"].append(val_stats["cls"])
        history["val_reg"].append(val_stats["reg"])

        # ---- EVALUATE mAP ----
        map_stats = evaluate_map(model, val_loader, device)
        history["mAP50"].append(map_stats["mAP50"])
        history["mAP50_95"].append(map_stats["mAP50_95"])

        print(f" Train - Total={train_stats['total']:.4f}, Cls={train_stats['cls']:.4f}, Reg={train_stats['reg']:.4f}")
        print(f" Val   - Total={val_stats['total']:.4f}, Cls={val_stats['cls']:.4f}, Reg={val_stats['reg']:.4f}")
        print(f" Eval  - mAP50={map_stats['mAP50']:.4f}, mAP50-95={map_stats['mAP50_95']:.4f}")

        # ---- SAVE CHECKPOINT EVERY EPOCH ----
        ckpt_path = os.path.join(exp_dir, f"checkpoint_epoch_{epoch+1}.pth")
        save_checkpoint(model, optimizer, epoch+1, history, ckpt_path)

        # ---- SAVE BEST CHECKPOINT BY mAP50 ----
        if map_stats["mAP50"] > best_map50:
            best_map50 = map_stats["mAP50"]
            best_path = os.path.join(exp_dir, "best_map50.pth")
            save_checkpoint(model, optimizer, epoch+1, history, best_path)
            print(f"ðŸ”¥ Saved BEST checkpoint by mAP50 = {best_map50:.4f}")

    return model, history

device = "cuda" if torch.cuda.is_available() else "cpu"
num_classes = 14 + 1

model = get_retinanet(num_classes)

trained_model, history = train_model(
    model,
    train_loader,
    valid_loader,
    num_epochs=50,
    freeze_epochs=20
)

import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 3, figsize=(18, 10))
loss_names = ["total", "cls", "reg"]
titles = ["Total Loss", "Classification Loss", "Regression Loss"]

# --- Plot TRAIN losses ---
for i, loss in enumerate(loss_names):
    axes[0, i].plot(history[f"train_{loss}"], label=f"Train {titles[i]}", color='blue')
    axes[0, i].set_title(f"Train {titles[i]}")
    axes[0, i].set_xlabel("Epoch")
    axes[0, i].set_ylabel("Loss")
    axes[0, i].grid(True)

# --- Plot VALIDATION losses ---
for i, loss in enumerate(loss_names):
    axes[1, i].plot(history[f"val_{loss}"], label=f"Val {titles[i]}", color='orange')
    axes[1, i].set_title(f"Validation {titles[i]}")
    axes[1, i].set_xlabel("Epoch")
    axes[1, i].set_ylabel("Loss")
    axes[1, i].grid(True)

plt.tight_layout()
plt.show()

!zip -r /content/exp.zip /content/exp

from google.colab import drive
drive.mount('/content/drive')

!cp /content/exp.zip /content/drive/MyDrive/

def evaluate_after_training(num_classes, device, val_loader, model_path="checkpoints/best_model.pth"):
    print("\nLoading best model for mAP evaluation...")

    model = get_retinanet(num_classes)
    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)

    _, history = load_checkpoint(model, optimizer, model_path, device)

    model.to(device)
    model.eval()

    print("Computing mAP...")
    map_stats = evaluate_map(model, val_loader, device)

    print("\n===== FINAL mAP RESULT =====")
    print(f" mAP@50    : {map_stats['mAP50']:.4f}")
    print(f" mAP@50-95 : {map_stats['mAP50_95']:.4f}")

    return map_stats, history